#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›…æ€å£è¯­ Part 1 Anki å¡ç‰‡ç”Ÿæˆå™¨
ç”¨äºå¤„ç† Part1 æ–‡æœ¬ï¼Œè‡ªåŠ¨å»é‡ç›¸ä¼¼å¥å­ï¼Œç”Ÿæˆ Anki å¡ç‰‡
"""

import os
import sys
import json
import asyncio
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Tuple, Set, Optional
from difflib import SequenceMatcher
import openai
import edge_tts
import genanki
from dotenv import load_dotenv

# å°è¯•å¯¼å…¥ sentence-transformersï¼ˆç”¨äºè¯­ä¹‰å»é‡ï¼‰
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    SEMANTIC_AVAILABLE = True
except ImportError:
    SEMANTIC_AVAILABLE = False
    SentenceTransformer = None
    np = None


# ============= åŠ è½½ç¯å¢ƒå˜é‡ =============
load_dotenv()

# ============= é…ç½®é¡¹ =============
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
VOICE = "en-US-ChristopherNeural"  # Edge-TTS è¯­éŸ³
INPUT_FILE = Path("Part1æ–‡æœ¬.md")  # Part1 è¾“å…¥æ–‡ä»¶
OUTPUT_DIR = Path("output")  # è¾“å‡ºç›®å½•
TEMP_AUDIO_DIR = OUTPUT_DIR / "temp_audio_part1"  # ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ç›®å½•
OUTPUT_APKG = OUTPUT_DIR / "IELTS_Part1_Speaking.apkg"

# Anki Model ID (éšæœºç”Ÿæˆçš„å”¯ä¸€ID)
MODEL_ID = 1607392320
DECK_ID = 1607392321

# ============= ç›¸ä¼¼åº¦é˜ˆå€¼ =============
SIMILARITY_THRESHOLD = 0.75  # å¥å­ç›¸ä¼¼åº¦è¶…è¿‡æ­¤é˜ˆå€¼è®¤ä¸ºæ˜¯é‡å¤

# ============= å»é‡ç®—æ³•é…ç½® =============
USE_SEMANTIC_DEDUP = True  # ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å»é‡ï¼ˆéœ€è¦ sentence-transformersï¼‰
SEMANTIC_THRESHOLD = 0.75   # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼


def similarity(a: str, b: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
    
    Args:
        a: å­—ç¬¦ä¸²1
        b: å­—ç¬¦ä¸²2
        
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
    """
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def parse_part1_text(content: str) -> List[Dict[str, str]]:
    """
    è§£æ Part1 æ–‡æœ¬ï¼Œæå–è¯é¢˜ã€é—®é¢˜å’Œå›ç­”
    """
    qa_pairs = []
    lines = content.split('\n')
    
    current_topic = ""
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        # åŒ¹é…è¯é¢˜è¡Œ
        if line.startswith('è¯é¢˜:'):
            current_topic = line.replace('è¯é¢˜:', '').strip()
            i += 1
            continue
        
        # åŒ¹é…é—®é¢˜è¡Œ
        question_match = re.match(r'é—®é¢˜:\s*(\d+)\.\s*(.+)', line)
        if question_match:
            question_num = question_match.group(1)
            question_text = question_match.group(2).strip()
            
            # æŸ¥æ‰¾å¯¹åº”çš„å›ç­”
            answer_lines = []
            i += 1
            
            # è·³è¿‡ç©ºè¡Œå’Œå…³é”®è¯è¡Œï¼Œç›´åˆ°æ‰¾åˆ°"å›ç­”:"
            while i < len(lines) and not lines[i].strip().startswith('å›ç­”:'):
                i += 1
            
            # è·³è¿‡"å›ç­”:"è¡Œ
            if i < len(lines) and lines[i].strip().startswith('å›ç­”:'):
                i += 1
            
            # æ”¶é›†å›ç­”å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜ã€è¯é¢˜æˆ–åˆ†éš”ç¬¦ï¼‰
            while i < len(lines):
                current_line = lines[i]
                stripped = current_line.strip()
                
                # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜ã€è¯é¢˜æˆ–åˆ†éš”ç¬¦ï¼Œåœæ­¢
                if (re.match(r'é—®é¢˜:\s*\d+\.', stripped) or
                    stripped.startswith('è¯é¢˜:') or
                    stripped == '---'):
                    break
                
                # è·³è¿‡å…³é”®è¯è¡Œ
                if stripped.startswith('å…³é”®è¯:'):
                    i += 1
                    continue
                
                # åªæ·»åŠ éç©ºè¡Œ
                if stripped:
                    answer_lines.append(current_line)
                
                i += 1
            
            answer = '\n'.join(answer_lines).strip()
            
            if answer:
                qa_pairs.append({
                    'topic': current_topic,
                    'question_num': question_num,
                    'question': question_text,
                    'answer': answer
                })
            
            continue
        
        i += 1
    
    print(f"âœ“ è§£æåˆ° {len(qa_pairs)} ä¸ªé—®é¢˜-å›ç­”å¯¹")
    return qa_pairs


def normalize_sentence(s: str) -> str:
    """
    è§„èŒƒåŒ–å¥å­ä»¥ä¾¿æ¯”è¾ƒï¼šå°å†™ã€å»é™¤æ ‡ç‚¹ã€å»é™¤å¤šä½™ç©ºæ ¼
    """
    # è½¬æ¢ä¸ºå°å†™
    s = s.lower()
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼ï¼‰
    s = re.sub(r'[^\w\s]', '', s)
    # åˆå¹¶å¤šä½™ç©ºæ ¼
    s = re.sub(r'\s+', ' ', s).strip()
    return s


def deduplicate_sentences_string(qa_pairs: List[Dict[str, str]]) -> Tuple[List[Dict[str, str]], Dict[str, List[str]]]:
    """
    å»é™¤ç›¸ä¼¼çš„å¥å­ï¼Œè¿”å›å»é‡åçš„å¥å­åˆ—è¡¨å’Œæ¥æºæ˜ å°„

    Args:
        qa_pairs: é—®é¢˜-å›ç­”å¯¹åˆ—è¡¨

    Returns:
        (å»é‡åçš„å¥å­åˆ—è¡¨, æ¥æºæ˜ å°„)
    """
    all_sentences = []
    
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¥å­
    for qa in qa_pairs:
        # å°†å›ç­”æŒ‰å¥å­åˆ†å‰²
        answer = qa['answer']
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†å¥ï¼ˆä¿ç•™ç¼©å†™ï¼‰
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', answer)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        for sent in sentences:
            # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
            if len(sent.split()) < 3:
                continue
            
            all_sentences.append({
                'sentence': sent,
                'topic': qa['topic'],
                'question': qa['question'],
                'full_answer': answer
            })
    
    print(f"  å…±æå– {len(all_sentences)} ä¸ªåŸå§‹å¥å­")
    
    # å»é‡
    unique_sentences = []
    source_map = {}  # sentence -> list of sources
    normalized_set = set()  # ç”¨äºå¿«é€Ÿæ£€æµ‹å®Œå…¨é‡å¤
    
    for item in all_sentences:
        sent = item['sentence']
        normalized = normalize_sentence(sent)
        
        # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿å­˜çš„å¥å­ç›¸ä¼¼
        is_duplicate = False
        for existing in unique_sentences:
            existing_sent = existing['sentence']
            
            # å¦‚æœé•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—
            len_ratio = len(sent) / len(existing_sent) if existing_sent else 0
            if len_ratio < 0.7 or len_ratio > 1.43:
                continue
            
            sim = similarity(sent, existing_sent)
            if sim >= SIMILARITY_THRESHOLD:
                is_duplicate = True
                # è®°å½•æ¥æº
                key = existing_sent
                if key not in source_map:
                    source_map[key] = []
                source_map[key].append(f"{item['topic']} - {item['question']}")
                break
        
        # å¦‚æœæœªå‘ç°ç›¸ä¼¼ï¼Œä½†è§„èŒƒåŒ–åå®Œå…¨ç›¸åŒï¼Œä¹Ÿè§†ä¸ºé‡å¤
        if not is_duplicate and normalized in normalized_set:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å¥å­
            for existing in unique_sentences:
                if normalize_sentence(existing['sentence']) == normalized:
                    is_duplicate = True
                    key = existing['sentence']
                    if key not in source_map:
                        source_map[key] = []
                    source_map[key].append(f"{item['topic']} - {item['question']}")
                    break
        
        if not is_duplicate:
            unique_sentences.append(item)
            source_map[sent] = [f"{item['topic']} - {item['question']}"]
            normalized_set.add(normalized)
    
    print(f"  å»é‡åå‰©ä½™ {len(unique_sentences)} ä¸ªå¥å­")
    print(f"  å»é™¤äº† {len(all_sentences) - len(unique_sentences)} ä¸ªé‡å¤å¥å­")
    
    return unique_sentences, source_map


def deduplicate_sentences(qa_pairs: List[Dict[str, str]]) -> Tuple[List[Dict[str, str]], Dict[str, List[str]]]:
    """
    å»é‡å¥å­çš„ä¸»å‡½æ•°ï¼Œæ ¹æ®é…ç½®é€‰æ‹©ç®—æ³•
    """
    if USE_SEMANTIC_DEDUP and SEMANTIC_AVAILABLE:
        print("ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦å»é‡ç®—æ³•")
        return deduplicate_sentences_semantic(qa_pairs)
    else:
        if USE_SEMANTIC_DEDUP and not SEMANTIC_AVAILABLE:
            print("è­¦å‘Š: é…ç½®äº†ä½¿ç”¨è¯­ä¹‰å»é‡ä½† sentence-transformers ä¸å¯ç”¨ï¼Œå›é€€åˆ°å­—ç¬¦ä¸²ç›¸ä¼¼åº¦")
        print("ä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦å»é‡ç®—æ³•")
        return deduplicate_sentences_string(qa_pairs)


def deduplicate_sentences_semantic(qa_pairs: List[Dict[str, str]]) -> Tuple[List[Dict[str, str]], Dict[str, List[str]]]:
    """
    åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å»é‡ï¼ˆä½¿ç”¨ sentence-transformersï¼‰
    
    Args:
        qa_pairs: é—®é¢˜-å›ç­”å¯¹åˆ—è¡¨
        
    Returns:
        (å»é‡åçš„å¥å­åˆ—è¡¨, æ¥æºæ˜ å°„)
    """
    if not SEMANTIC_AVAILABLE:
        print("è­¦å‘Š: sentence-transformers ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸºäºå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦å»é‡")
        return deduplicate_sentences_string(qa_pairs)
    
    all_sentences = []
    
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¥å­
    for qa in qa_pairs:
        answer = qa['answer']
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†å¥ï¼ˆä¿ç•™ç¼©å†™ï¼‰
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', answer)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        for sent in sentences:
            # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
            if len(sent.split()) < 3:
                continue
            
            all_sentences.append({
                'sentence': sent,
                'topic': qa['topic'],
                'question': qa['question'],
                'full_answer': answer
            })
    
    print(f"  å…±æå– {len(all_sentences)} ä¸ªåŸå§‹å¥å­")
    
    # æå–å¥å­æ–‡æœ¬
    sentence_texts = [item['sentence'] for item in all_sentences]
    
    # åŠ è½½æ¨¡å‹ï¼ˆæƒ°æ€§åŠ è½½ï¼‰
    assert SEMANTIC_AVAILABLE and SentenceTransformer is not None
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(sentence_texts, convert_to_tensor=False)
    
    # ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è¾…åŠ©å‡½æ•°
    def cosine_sim(a, b):
        from numpy import dot
        from numpy.linalg import norm
        return dot(a, b) / (norm(a) * norm(b))
    
    # å»é‡
    unique_sentences = []
    unique_embeddings = []
    source_map = {}
    
    for idx, (item, emb) in enumerate(zip(all_sentences, embeddings)):
        sent = item['sentence']
        duplicate = False
        for j, (u_item, u_emb) in enumerate(zip(unique_sentences, unique_embeddings)):
            sim = cosine_sim(emb, u_emb)
            if sim >= SEMANTIC_THRESHOLD:
                duplicate = True
                key = u_item['sentence']
                if key not in source_map:
                    source_map[key] = []
                source_map[key].append(f"{item['topic']} - {item['question']}")
                break
        if not duplicate:
            unique_sentences.append(item)
            unique_embeddings.append(emb)
            source_map[sent] = [f"{item['topic']} - {item['question']}"]
    
    print(f"  å»é‡åå‰©ä½™ {len(unique_sentences)} ä¸ªå¥å­")
    print(f"  å»é™¤äº† {len(all_sentences) - len(unique_sentences)} ä¸ªé‡å¤å¥å­")
    
    return unique_sentences, source_map




def parse_text_with_ai(text: str, max_retries: int = 3) -> List[Dict[str, str]]:
    """
    ä½¿ç”¨ DeepSeek API å°†æ–‡æœ¬æ‹†è§£ä¸ºå¥å­ï¼Œå¹¶ç”Ÿæˆä¸­æ–‡ç¿»è¯‘å’Œå…³é”®è¯æç¤º

    Args:
        text: åŸå§‹è‹±æ–‡æ–‡æœ¬
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« english, chinese, keywords
    """
    # æ£€æŸ¥ API Key
    if not DEEPSEEK_API_KEY:
        print("âœ— é”™è¯¯: æœªè®¾ç½® DEEPSEEK_API_KEY")
        print("  è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DEEPSEEK_API_KEY=your-api-key-here")
        sys.exit(1)
    
    client = openai.OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL
    )
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé›…æ€å£è¯­æ•™å­¦åŠ©æ‰‹ã€‚è¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬æ‹†è§£æˆç‹¬ç«‹çš„å¥å­ï¼Œå¹¶ä¸ºæ¯ä¸ªå¥å­æä¾›ï¼š
1. english: åŸè‹±æ–‡å¥å­
2. chinese: ä¸­æ–‡ç¿»è¯‘
3. keywords: 3-5ä¸ªè‹±è¯­å…³é”®è¯æç¤ºï¼ˆç”¨äºå¸®åŠ©å›å¿†å¥å­ï¼‰

**é‡è¦**: è¯·åªè¿”å›çº¯JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•Markdownæ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰æˆ–å…¶ä»–æ–‡å­—è¯´æ˜ã€‚

è¾“å…¥æ–‡æœ¬ï¼š
{text}

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{"english": "Li Hua is a student.", "chinese": "æåæ˜¯ä¸€åå­¦ç”Ÿã€‚", "keywords": "Li Hua, student"}},
  {{"english": "He likes basketball.", "chinese": "ä»–å–œæ¬¢ç¯®çƒã€‚", "keywords": "he, likes, basketball"}}
]
"""
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that returns only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            
            if content is None:
                raise ValueError("API è¿”å›å†…å®¹ä¸ºç©º")
            
            content = content.strip()
            
            # ç§»é™¤å¯èƒ½çš„ Markdown ä»£ç å—æ ‡è®°
            content = re.sub(r'^```json\s*', '', content)
            content = re.sub(r'^```\s*', '', content)
            content = re.sub(r'\s*```$', '', content)
            
            # å°è¯•æå– JSON æ•°ç»„ï¼ˆå¤„ç†å¯èƒ½çš„å¤šä½™æ–‡æœ¬ï¼‰
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª '[' å’Œæœ€åä¸€ä¸ª ']'
            start = content.find('[')
            end = content.rfind(']')
            if start != -1 and end != -1 and end > start:
                json_str = content[start:end+1]
            else:
                json_str = content
            
            # è§£æ JSON
            data = json.loads(json_str)
            
            # éªŒè¯æ•°æ®ç»“æ„
            if not isinstance(data, list):
                raise ValueError("API è¿”å›çš„ä¸æ˜¯ JSON æ•°ç»„")
            
            for item in data:
                if not all(key in item for key in ('english', 'chinese', 'keywords')):
                    raise ValueError("API è¿”å›çš„ JSON ç¼ºå°‘å¿…è¦å­—æ®µ")
            
            print(f"âœ“ æˆåŠŸè§£æ {len(data)} ä¸ªå¥å­")
            return data
            
        except json.JSONDecodeError as e:
            print(f"âœ— JSON è§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                print(f"API è¿”å›å†…å®¹:\n{content}")
                raise
            # ç­‰å¾…åé‡è¯•
            import time
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        except Exception as e:
            print(f"âœ— API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise
            import time
            time.sleep(2 ** attempt)
    
    # ä¸åº”åˆ°è¾¾è¿™é‡Œ
    raise RuntimeError("é‡è¯•æ¬¡æ•°ç”¨å°½")


async def generate_audio(text: str, filename: Path) -> bool:
    """
    ä½¿ç”¨ Edge-TTS ç”Ÿæˆè‹±æ–‡è¯­éŸ³
    
    Args:
        text: è¦è½¬æ¢çš„è‹±æ–‡æ–‡æœ¬
        filename: è¾“å‡ºçš„ MP3 æ–‡ä»¶è·¯å¾„
        
    Returns:
        æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    try:
        communicate = edge_tts.Communicate(text, VOICE)
        await communicate.save(str(filename))
        print(f"  âœ“ ç”ŸæˆéŸ³é¢‘: {filename.name}")
        return True
    except Exception as e:
        print(f"  âœ— éŸ³é¢‘ç”Ÿæˆå¤±è´¥ ({filename.name}): {e}")
        # åˆ é™¤å¯èƒ½åˆ›å»ºçš„ç©ºæ–‡ä»¶
        if filename.exists():
            try:
                filename.unlink()
            except:
                pass
        return False


async def generate_audio_with_retry(text: str, filename: Path, max_retries: int = 3) -> bool:
    """
    å¸¦é‡è¯•çš„éŸ³é¢‘ç”Ÿæˆ
    
    Args:
        text: è¦è½¬æ¢çš„è‹±æ–‡æ–‡æœ¬
        filename: è¾“å‡ºçš„ MP3 æ–‡ä»¶è·¯å¾„
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
    Returns:
        æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    for attempt in range(max_retries):
        try:
            success = await generate_audio(text, filename)
            if success:
                return True
            else:
                # generate_audio å†…éƒ¨å·²æ‰“å°é”™è¯¯ï¼Œè¿™é‡Œåªè®°å½•é‡è¯•
                if attempt < max_retries - 1:
                    print(f"  é‡è¯• {attempt + 1}/{max_retries}...")
                    await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
        except Exception as e:
            print(f"  é‡è¯• {attempt + 1}/{max_retries} å¼‚å¸¸: {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(1)
    
    print(f"  âœ— éŸ³é¢‘ç”Ÿæˆé‡è¯• {max_retries} æ¬¡åä»å¤±è´¥: {filename.name}")
    return False


async def generate_all_audio(sentences: List[Dict[str, str]]) -> List[Optional[Path]]:
    """
    æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å¥å­çš„éŸ³é¢‘æ–‡ä»¶
    
    Args:
        sentences: å¥å­æ•°æ®åˆ—è¡¨
        
    Returns:
        ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¤±è´¥çš„ä½ç½®ä¸º Noneï¼‰
    """
    TEMP_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
    
    # é™åˆ¶å¹¶å‘æ•°ï¼Œé¿å…è¿‡å¤šè¯·æ±‚å¯¼è‡´å¤±è´¥
    CONCURRENT_LIMIT = 3  # è¿›ä¸€æ­¥é™ä½å¹¶å‘
    semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)
    
    # è®°å½•å¤±è´¥å¥å­åˆ°æ–‡ä»¶
    failed_log = OUTPUT_DIR / "audio_failed_sentences.txt"
    failed_sentences = []
    
    async def generate_with_limit(sentence: Dict[str, str], idx: int) -> Optional[Path]:
        """å¸¦å¹¶å‘é™åˆ¶çš„éŸ³é¢‘ç”Ÿæˆä»»åŠ¡"""
        sent_hash = hashlib.md5(sentence['english'].encode()).hexdigest()[:8]
        audio_file = TEMP_AUDIO_DIR / f"part1_{sent_hash}.mp3"
        
        async with semaphore:
            # åœ¨è¯·æ±‚ä¹‹é—´æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…é€Ÿç‡é™åˆ¶
            await asyncio.sleep(0.3)
            success = await generate_audio_with_retry(sentence["english"], audio_file, max_retries=2)
        
        if success:
            return audio_file
        else:
            # è®°å½•å¤±è´¥å¥å­
            print(f"  âœ— å¥å­ {idx+1} éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {sentence['english'][:50]}...")
            failed_sentences.append({
                'index': idx,
                'english': sentence['english'],
                'chinese': sentence['chinese'],
                'keywords': sentence['keywords']
            })
            return None
    
    print(f"\nå¼€å§‹ç”Ÿæˆ {len(sentences)} ä¸ªéŸ³é¢‘æ–‡ä»¶ (å¹¶å‘é™åˆ¶: {CONCURRENT_LIMIT})...")
    
    # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
    tasks = [generate_with_limit(sentence, idx) for idx, sentence in enumerate(sentences)]
    results = await asyncio.gather(*tasks)
    
    # ç»Ÿè®¡ç»“æœ
    successful_audio_files = []
    failed_count = 0
    for result in results:
        if result is None:
            successful_audio_files.append(None)
            failed_count += 1
        else:
            successful_audio_files.append(result)
    
    success_count = len(sentences) - failed_count
    print(f"âœ“ éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª\n")
    
    # ä¿å­˜å¤±è´¥å¥å­æ—¥å¿—
    if failed_sentences:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        with open(failed_log, 'w', encoding='utf-8') as f:
            f.write(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥å¥å­ ({len(failed_sentences)} ä¸ª):\n\n")
            for item in failed_sentences:
                f.write(f"ç´¢å¼•: {item['index']}\n")
                f.write(f"è‹±æ–‡: {item['english']}\n")
                f.write(f"ä¸­æ–‡: {item['chinese']}\n")
                f.write(f"å…³é”®è¯: {item['keywords']}\n")
                f.write("-" * 80 + "\n")
        print(f"  å¤±è´¥å¥å­æ—¥å¿—å·²ä¿å­˜åˆ°: {failed_log}")
    
    return successful_audio_files


def create_anki_deck(sentences: List[Dict[str, str]], audio_files: List[Optional[Path]]) -> str:
    """
    åˆ›å»º Anki å¡ç‰‡åŒ…
    
    Args:
        sentences: å¥å­æ•°æ®åˆ—è¡¨
        audio_files: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå¯èƒ½åŒ…å« Noneï¼‰
        
    Returns:
        ç”Ÿæˆçš„ .apkg æ–‡ä»¶è·¯å¾„
    """
    # å®šä¹‰å¡ç‰‡æ¨¡æ¿
    model = genanki.Model(
        MODEL_ID,
        'IELTS Part1 Speaking Model',
        fields=[
            {'name': 'Chinese'},
            {'name': 'Keywords'},
            {'name': 'English'},
            {'name': 'Audio'},
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '''
                    <div style="font-family: Arial; font-size: 24px; text-align: center; margin: 20px;">
                        {{Chinese}}
                    </div>
                    <div style="font-size: 14px; color: #888; text-align: center; margin-top: 15px;">
                        <i>ğŸ’¡ æç¤º: {{Keywords}}</i>
                    </div>
                ''',
                'afmt': '''
                    <div style="font-family: Arial; font-size: 24px; text-align: center; margin: 20px;">
                        {{Chinese}}
                    </div>
                    <div style="font-size: 14px; color: #888; text-align: center; margin-top: 15px;">
                        <i>ğŸ’¡ æç¤º: {{Keywords}}</i>
                    </div>
                    <hr>
                    <div style="font-size: 20px; color: #333; text-align: center; margin: 20px;">
                        {{English}}
                    </div>
                    <div style="text-align: center; margin-top: 15px;">
                        {{Audio}}
                    </div>
                ''',
            },
        ],
        css='''
            .card {
                font-family: Arial, sans-serif;
                background-color: #f9f9f9;
                padding: 20px;
            }
        '''
    )
    
    # åˆ›å»º Deck
    deck = genanki.Deck(DECK_ID, "IELTS Speaking Part 1")
    
    # åˆ›å»º Package
    package = genanki.Package(deck)
    
    print("å¼€å§‹åˆ›å»º Anki å¡ç‰‡...")
    
    # æ·»åŠ å¡ç‰‡
    cards_with_audio = 0
    cards_without_audio = 0
    for idx, (sentence, audio_file) in enumerate(zip(sentences, audio_files)):
        # éŸ³é¢‘å­—æ®µ
        audio_field = f'[sound:{audio_file.name}]' if audio_file is not None else ''
        # åˆ›å»º Note
        note = genanki.Note(
            model=model,
            fields=[
                sentence['chinese'],
                sentence['keywords'],
                sentence['english'],
                audio_field
            ]
        )
        deck.add_note(note)
        
        # æ·»åŠ éŸ³é¢‘æ–‡ä»¶åˆ° Packageï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if audio_file is not None:
            package.media_files.append(str(audio_file))
            cards_with_audio += 1
        else:
            cards_without_audio += 1
        
        if (idx + 1) % 10 == 0 or idx == len(sentences) - 1:
            print(f"  âœ“ æ·»åŠ å¥å­å¡ç‰‡ {idx + 1}/{len(sentences)}")
    
    # å¯¼å‡º .apkg æ–‡ä»¶
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    package.write_to_file(str(OUTPUT_APKG))
    print(f"\nâœ“ æˆåŠŸç”Ÿæˆ Anki åŒ…: {OUTPUT_APKG}")
    print(f"  - {len(sentences)} å¼ å¥å­å¡ç‰‡ï¼ˆ{cards_with_audio} å¼ å¸¦éŸ³é¢‘ï¼Œ{cards_without_audio} å¼ æ— éŸ³é¢‘ï¼‰")
    
    return str(OUTPUT_APKG)


def cleanup_temp_files():
    """åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶"""
    if TEMP_AUDIO_DIR.exists():
        for file in TEMP_AUDIO_DIR.glob("*.mp3"):
            try:
                file.unlink()
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ {file}: {e}")
        
        try:
            TEMP_AUDIO_DIR.rmdir()
            print("âœ“ ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½•: {e}")


async def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    print("=" * 60)
    print("é›…æ€å£è¯­ Part 1 Anki å¡ç‰‡ç”Ÿæˆå™¨".center(60))
    print("=" * 60)
    print()
    
    try:
        # Step 0: è¯»å–è¾“å…¥æ–‡ä»¶
        print("ğŸ“„ Step 0: è¯»å– Part1 æ–‡æœ¬æ–‡ä»¶...")
        if not INPUT_FILE.exists():
            print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{INPUT_FILE}'")
            sys.exit(1)
        
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"âœ“ æˆåŠŸè¯»å–è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        print(f"  æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print()
        
        # Step 1: è§£ææ–‡æœ¬ï¼Œæå–é—®é¢˜å’Œå›ç­”
        print("ğŸ“ Step 1: è§£æ Part1 æ–‡æœ¬ç»“æ„...")
        qa_pairs = parse_part1_text(content)
        print()
        
        # Step 2: å»é‡ç›¸ä¼¼å¥å­
        print("ğŸ” Step 2: æ£€æµ‹å¹¶å»é™¤ç›¸ä¼¼å¥å­...")
        unique_items, source_map = deduplicate_sentences(qa_pairs)
        print()
        
        # åˆå¹¶æ‰€æœ‰ç‹¬ç‰¹å¥å­ç”¨äº AI å¤„ç†
        combined_text = " ".join([item['sentence'] for item in unique_items])
        
        # Step 3: è°ƒç”¨ AI æ‹†è§£å¥å­
        print("ğŸ¤– Step 3: ä½¿ç”¨ DeepSeek API æ‹†è§£å¥å­...")
        # ä¸ºäº†æ•ˆç‡ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š 10 ä¸ªå¥å­ï¼‰
        batch_size = 10
        all_parsed_sentences = []
        
        for i in range(0, len(unique_items), batch_size):
            batch = unique_items[i:i+batch_size]
            batch_text = " ".join([item['sentence'] for item in batch])
            
            print(f"\n  å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(unique_items) + batch_size - 1)//batch_size}...")
            parsed = parse_text_with_ai(batch_text)
            all_parsed_sentences.extend(parsed)
        
        print(f"\nâœ“ å…±è§£æ {len(all_parsed_sentences)} ä¸ªå¥å­")
        print()
        
        # Step 4: ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        print("ğŸ”Š Step 4: ä½¿ç”¨ Edge-TTS ç”Ÿæˆè¯­éŸ³æ–‡ä»¶...")
        audio_files = await generate_all_audio(all_parsed_sentences)
        
        # Step 5: åˆ›å»º Anki å¡ç‰‡åŒ…
        print("ğŸ“¦ Step 5: ç”Ÿæˆ Anki å¡ç‰‡åŒ…...")
        apkg_file = create_anki_deck(all_parsed_sentences, audio_files)
        
        print()
        print("=" * 60)
        print("âœ… å…¨éƒ¨å®Œæˆï¼".center(60))
        print(f"è¾“å‡ºæ–‡ä»¶: {apkg_file}".center(60))
        print("=" * 60)
        
    except Exception as e:
        print()
        print("=" * 60)
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("=" * 60)
        raise
    
    finally:
        # Step 6: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print()
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleanup_temp_files()


# ============= ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    asyncio.run(main())