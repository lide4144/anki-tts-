#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›…æ€å£è¯­ Part 1 Anki å¡ç‰‡ç”Ÿæˆå™¨
ç”¨äºå¤„ç† Part1 æ–‡æœ¬ï¼Œè‡ªåŠ¨å»é‡ç›¸ä¼¼å¥å­ï¼Œç”Ÿæˆ Anki å¡ç‰‡
"""

import os
import sys
import json
import asyncio
import re
import hashlib
from pathlib import Path
from typing import List, Dict, Tuple, Set
from difflib import SequenceMatcher
import openai
import edge_tts
import genanki
from dotenv import load_dotenv


# ============= åŠ è½½ç¯å¢ƒå˜é‡ =============
load_dotenv()

# ============= é…ç½®é¡¹ =============
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
VOICE = "en-US-ChristopherNeural"  # Edge-TTS è¯­éŸ³
INPUT_FILE = Path("Part1æ–‡æœ¬.md")  # Part1 è¾“å…¥æ–‡ä»¶
OUTPUT_DIR = Path("output")  # è¾“å‡ºç›®å½•
TEMP_AUDIO_DIR = OUTPUT_DIR / "temp_audio_part1"  # ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶ç›®å½•
OUTPUT_APKG = OUTPUT_DIR / "IELTS_Part1_Speaking.apkg"

# Anki Model ID (éšæœºç”Ÿæˆçš„å”¯ä¸€ID)
MODEL_ID = 1607392320
DECK_ID = 1607392321

# ============= ç›¸ä¼¼åº¦é˜ˆå€¼ =============
SIMILARITY_THRESHOLD = 0.75  # å¥å­ç›¸ä¼¼åº¦è¶…è¿‡æ­¤é˜ˆå€¼è®¤ä¸ºæ˜¯é‡å¤


def similarity(a: str, b: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
    
    Args:
        a: å­—ç¬¦ä¸²1
        b: å­—ç¬¦ä¸²2
        
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0.0 - 1.0)
    """
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()


def parse_part1_text(content: str) -> List[Dict[str, str]]:
    """
    è§£æ Part1 æ–‡æœ¬ï¼Œæå–è¯é¢˜ã€é—®é¢˜å’Œå›ç­”
    """
    qa_pairs = []
    lines = content.split('\n')
    
    current_topic = ""
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        # åŒ¹é…è¯é¢˜è¡Œ
        if line.startswith('è¯é¢˜:'):
            current_topic = line.replace('è¯é¢˜:', '').strip()
            i += 1
            continue
        
        # åŒ¹é…é—®é¢˜è¡Œ
        question_match = re.match(r'é—®é¢˜:\s*(\d+)\.\s*(.+)', line)
        if question_match:
            question_num = question_match.group(1)
            question_text = question_match.group(2).strip()
            
            # æŸ¥æ‰¾å¯¹åº”çš„å›ç­”
            answer_lines = []
            i += 1
            
            # è·³è¿‡ç©ºè¡Œå’Œå…³é”®è¯è¡Œï¼Œç›´åˆ°æ‰¾åˆ°"å›ç­”:"
            while i < len(lines) and not lines[i].strip().startswith('å›ç­”:'):
                i += 1
            
            # è·³è¿‡"å›ç­”:"è¡Œ
            if i < len(lines) and lines[i].strip().startswith('å›ç­”:'):
                i += 1
            
            # æ”¶é›†å›ç­”å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜ã€è¯é¢˜æˆ–åˆ†éš”ç¬¦ï¼‰
            while i < len(lines):
                current_line = lines[i]
                stripped = current_line.strip()
                
                # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªé—®é¢˜ã€è¯é¢˜æˆ–åˆ†éš”ç¬¦ï¼Œåœæ­¢
                if (re.match(r'é—®é¢˜:\s*\d+\.', stripped) or
                    stripped.startswith('è¯é¢˜:') or
                    stripped == '---'):
                    break
                
                # è·³è¿‡å…³é”®è¯è¡Œ
                if stripped.startswith('å…³é”®è¯:'):
                    i += 1
                    continue
                
                # åªæ·»åŠ éç©ºè¡Œ
                if stripped:
                    answer_lines.append(current_line)
                
                i += 1
            
            answer = '\n'.join(answer_lines).strip()
            
            if answer:
                qa_pairs.append({
                    'topic': current_topic,
                    'question_num': question_num,
                    'question': question_text,
                    'answer': answer
                })
            
            continue
        
        i += 1
    
    print(f"âœ“ è§£æåˆ° {len(qa_pairs)} ä¸ªé—®é¢˜-å›ç­”å¯¹")
    return qa_pairs


def deduplicate_sentences(qa_pairs: List[Dict[str, str]]) -> Tuple[List[Dict[str, str]], Dict[str, List[str]]]:
    """
    å»é™¤ç›¸ä¼¼çš„å¥å­ï¼Œè¿”å›å»é‡åçš„å¥å­åˆ—è¡¨å’Œæ¥æºæ˜ å°„
    
    Args:
        qa_pairs: é—®é¢˜-å›ç­”å¯¹åˆ—è¡¨
        
    Returns:
        (å»é‡åçš„å¥å­åˆ—è¡¨, æ¥æºæ˜ å°„)
    """
    all_sentences = []
    
    # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¥å­
    for qa in qa_pairs:
        # å°†å›ç­”æŒ‰å¥å­åˆ†å‰²
        answer = qa['answer']
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åˆ†å¥ï¼ˆä¿ç•™ç¼©å†™ï¼‰
        sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', answer)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        for sent in sentences:
            # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
            if len(sent.split()) < 3:
                continue
            
            all_sentences.append({
                'sentence': sent,
                'topic': qa['topic'],
                'question': qa['question'],
                'full_answer': answer
            })
    
    print(f"  å…±æå– {len(all_sentences)} ä¸ªåŸå§‹å¥å­")
    
    # å»é‡
    unique_sentences = []
    source_map = {}  # sentence -> list of sources
    
    for item in all_sentences:
        sent = item['sentence']
        
        # æ£€æŸ¥æ˜¯å¦ä¸å·²ä¿å­˜çš„å¥å­ç›¸ä¼¼
        is_duplicate = False
        for existing in unique_sentences:
            existing_sent = existing['sentence']
            
            # å¦‚æœé•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œè·³è¿‡ç›¸ä¼¼åº¦è®¡ç®—
            len_ratio = len(sent) / len(existing_sent) if existing_sent else 0
            if len_ratio < 0.7 or len_ratio > 1.43:
                continue
            
            sim = similarity(sent, existing_sent)
            if sim >= SIMILARITY_THRESHOLD:
                is_duplicate = True
                # è®°å½•æ¥æº
                key = existing_sent
                if key not in source_map:
                    source_map[key] = []
                source_map[key].append(f"{item['topic']} - {item['question']}")
                break
        
        if not is_duplicate:
            unique_sentences.append(item)
            source_map[sent] = [f"{item['topic']} - {item['question']}"]
    
    print(f"  å»é‡åå‰©ä½™ {len(unique_sentences)} ä¸ªå¥å­")
    print(f"  å»é™¤äº† {len(all_sentences) - len(unique_sentences)} ä¸ªé‡å¤å¥å­")
    
    return unique_sentences, source_map


def parse_text_with_ai(text: str) -> List[Dict[str, str]]:
    """
    ä½¿ç”¨ DeepSeek API å°†æ–‡æœ¬æ‹†è§£ä¸ºå¥å­ï¼Œå¹¶ç”Ÿæˆä¸­æ–‡ç¿»è¯‘å’Œå…³é”®è¯æç¤º
    
    Args:
        text: åŸå§‹è‹±æ–‡æ–‡æœ¬
        
    Returns:
        å¥å­åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« english, chinese, keywords
    """
    # æ£€æŸ¥ API Key
    if not DEEPSEEK_API_KEY:
        print("âœ— é”™è¯¯: æœªè®¾ç½® DEEPSEEK_API_KEY")
        print("  è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DEEPSEEK_API_KEY=your-api-key-here")
        sys.exit(1)
    
    client = openai.OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL
    )
    
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé›…æ€å£è¯­æ•™å­¦åŠ©æ‰‹ã€‚è¯·å°†ä¸‹é¢çš„è‹±æ–‡æ–‡æœ¬æ‹†è§£æˆç‹¬ç«‹çš„å¥å­ï¼Œå¹¶ä¸ºæ¯ä¸ªå¥å­æä¾›ï¼š
1. english: åŸè‹±æ–‡å¥å­
2. chinese: ä¸­æ–‡ç¿»è¯‘
3. keywords: 3-5ä¸ªè‹±è¯­å…³é”®è¯æç¤ºï¼ˆç”¨äºå¸®åŠ©å›å¿†å¥å­ï¼‰

**é‡è¦**: è¯·åªè¿”å›çº¯JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«ä»»ä½•Markdownæ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰æˆ–å…¶ä»–æ–‡å­—è¯´æ˜ã€‚

è¾“å…¥æ–‡æœ¬ï¼š
{text}

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
[
  {{"english": "Li Hua is a student.", "chinese": "æåæ˜¯ä¸€åå­¦ç”Ÿã€‚", "keywords": "Li Hua, student"}},
  {{"english": "He likes basketball.", "chinese": "ä»–å–œæ¬¢ç¯®çƒã€‚", "keywords": "he, likes, basketball"}}
]
"""
    
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that returns only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        
        content = response.choices[0].message.content
        
        if content is None:
            raise ValueError("API è¿”å›å†…å®¹ä¸ºç©º")
        
        content = content.strip()
        
        # ç§»é™¤å¯èƒ½çš„ Markdown ä»£ç å—æ ‡è®°
        content = re.sub(r'^```json\s*', '', content)
        content = re.sub(r'^```\s*', '', content)
        content = re.sub(r'\s*```$', '', content)
        
        # è§£æ JSON
        data = json.loads(content)
        
        print(f"âœ“ æˆåŠŸè§£æ {len(data)} ä¸ªå¥å­")
        return data
        
    except json.JSONDecodeError as e:
        print(f"âœ— JSON è§£æå¤±è´¥: {e}")
        print(f"API è¿”å›å†…å®¹:\n{content}")
        raise
    except Exception as e:
        print(f"âœ— API è°ƒç”¨å¤±è´¥: {e}")
        raise


async def generate_audio(text: str, filename: Path) -> None:
    """
    ä½¿ç”¨ Edge-TTS ç”Ÿæˆè‹±æ–‡è¯­éŸ³
    
    Args:
        text: è¦è½¬æ¢çš„è‹±æ–‡æ–‡æœ¬
        filename: è¾“å‡ºçš„ MP3 æ–‡ä»¶è·¯å¾„
    """
    try:
        communicate = edge_tts.Communicate(text, VOICE)
        await communicate.save(str(filename))
        print(f"  âœ“ ç”ŸæˆéŸ³é¢‘: {filename.name}")
    except Exception as e:
        print(f"  âœ— éŸ³é¢‘ç”Ÿæˆå¤±è´¥ ({filename.name}): {e}")
        raise


async def generate_all_audio(sentences: List[Dict[str, str]]) -> List[Path]:
    """
    æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å¥å­çš„éŸ³é¢‘æ–‡ä»¶
    
    Args:
        sentences: å¥å­æ•°æ®åˆ—è¡¨
        
    Returns:
        ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    TEMP_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
    
    tasks = []
    audio_files = []
    
    for idx, sentence in enumerate(sentences):
        # ä½¿ç”¨å¥å­å†…å®¹ç”Ÿæˆå“ˆå¸Œä½œä¸ºæ–‡ä»¶åï¼Œé¿å…é‡å¤
        sent_hash = hashlib.md5(sentence['english'].encode()).hexdigest()[:8]
        audio_file = TEMP_AUDIO_DIR / f"part1_{sent_hash}.mp3"
        audio_files.append(audio_file)
        tasks.append(generate_audio(sentence["english"], audio_file))
    
    print(f"\nå¼€å§‹ç”Ÿæˆ {len(tasks)} ä¸ªéŸ³é¢‘æ–‡ä»¶...")
    await asyncio.gather(*tasks)
    print("âœ“ æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ç”Ÿæˆå®Œæˆ\n")
    
    return audio_files


def create_anki_deck(sentences: List[Dict[str, str]], audio_files: List[Path]) -> str:
    """
    åˆ›å»º Anki å¡ç‰‡åŒ…
    
    Args:
        sentences: å¥å­æ•°æ®åˆ—è¡¨
        audio_files: éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        ç”Ÿæˆçš„ .apkg æ–‡ä»¶è·¯å¾„
    """
    # å®šä¹‰å¡ç‰‡æ¨¡æ¿
    model = genanki.Model(
        MODEL_ID,
        'IELTS Part1 Speaking Model',
        fields=[
            {'name': 'Chinese'},
            {'name': 'Keywords'},
            {'name': 'English'},
            {'name': 'Audio'},
        ],
        templates=[
            {
                'name': 'Card 1',
                'qfmt': '''
                    <div style="font-family: Arial; font-size: 24px; text-align: center; margin: 20px;">
                        {{Chinese}}
                    </div>
                    <div style="font-size: 14px; color: #888; text-align: center; margin-top: 15px;">
                        <i>ğŸ’¡ æç¤º: {{Keywords}}</i>
                    </div>
                ''',
                'afmt': '''
                    <div style="font-family: Arial; font-size: 24px; text-align: center; margin: 20px;">
                        {{Chinese}}
                    </div>
                    <div style="font-size: 14px; color: #888; text-align: center; margin-top: 15px;">
                        <i>ğŸ’¡ æç¤º: {{Keywords}}</i>
                    </div>
                    <hr>
                    <div style="font-size: 20px; color: #333; text-align: center; margin: 20px;">
                        {{English}}
                    </div>
                    <div style="text-align: center; margin-top: 15px;">
                        {{Audio}}
                    </div>
                ''',
            },
        ],
        css='''
            .card {
                font-family: Arial, sans-serif;
                background-color: #f9f9f9;
                padding: 20px;
            }
        '''
    )
    
    # åˆ›å»º Deck
    deck = genanki.Deck(DECK_ID, "IELTS Speaking Part 1")
    
    # åˆ›å»º Package
    package = genanki.Package(deck)
    
    print("å¼€å§‹åˆ›å»º Anki å¡ç‰‡...")
    
    # æ·»åŠ å¡ç‰‡
    for idx, (sentence, audio_file) in enumerate(zip(sentences, audio_files)):
        # åˆ›å»º Note
        note = genanki.Note(
            model=model,
            fields=[
                sentence['chinese'],
                sentence['keywords'],
                sentence['english'],
                f'[sound:{audio_file.name}]'
            ]
        )
        deck.add_note(note)
        
        # æ·»åŠ éŸ³é¢‘æ–‡ä»¶åˆ° Package
        package.media_files.append(str(audio_file))
        
        if (idx + 1) % 10 == 0 or idx == len(sentences) - 1:
            print(f"  âœ“ æ·»åŠ å¥å­å¡ç‰‡ {idx + 1}/{len(sentences)}")
    
    # å¯¼å‡º .apkg æ–‡ä»¶
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    package.write_to_file(str(OUTPUT_APKG))
    print(f"\nâœ“ æˆåŠŸç”Ÿæˆ Anki åŒ…: {OUTPUT_APKG}")
    print(f"  - {len(sentences)} å¼ å¥å­å¡ç‰‡")
    
    return str(OUTPUT_APKG)


def cleanup_temp_files():
    """åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶"""
    if TEMP_AUDIO_DIR.exists():
        for file in TEMP_AUDIO_DIR.glob("*.mp3"):
            try:
                file.unlink()
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ {file}: {e}")
        
        try:
            TEMP_AUDIO_DIR.rmdir()
            print("âœ“ ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶ç›®å½•: {e}")


async def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    print("=" * 60)
    print("é›…æ€å£è¯­ Part 1 Anki å¡ç‰‡ç”Ÿæˆå™¨".center(60))
    print("=" * 60)
    print()
    
    try:
        # Step 0: è¯»å–è¾“å…¥æ–‡ä»¶
        print("ğŸ“„ Step 0: è¯»å– Part1 æ–‡æœ¬æ–‡ä»¶...")
        if not INPUT_FILE.exists():
            print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{INPUT_FILE}'")
            sys.exit(1)
        
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"âœ“ æˆåŠŸè¯»å–è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        print(f"  æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print()
        
        # Step 1: è§£ææ–‡æœ¬ï¼Œæå–é—®é¢˜å’Œå›ç­”
        print("ğŸ“ Step 1: è§£æ Part1 æ–‡æœ¬ç»“æ„...")
        qa_pairs = parse_part1_text(content)
        print()
        
        # Step 2: å»é‡ç›¸ä¼¼å¥å­
        print("ğŸ” Step 2: æ£€æµ‹å¹¶å»é™¤ç›¸ä¼¼å¥å­...")
        unique_items, source_map = deduplicate_sentences(qa_pairs)
        print()
        
        # åˆå¹¶æ‰€æœ‰ç‹¬ç‰¹å¥å­ç”¨äº AI å¤„ç†
        combined_text = " ".join([item['sentence'] for item in unique_items])
        
        # Step 3: è°ƒç”¨ AI æ‹†è§£å¥å­
        print("ğŸ¤– Step 3: ä½¿ç”¨ DeepSeek API æ‹†è§£å¥å­...")
        # ä¸ºäº†æ•ˆç‡ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š 10 ä¸ªå¥å­ï¼‰
        batch_size = 10
        all_parsed_sentences = []
        
        for i in range(0, len(unique_items), batch_size):
            batch = unique_items[i:i+batch_size]
            batch_text = " ".join([item['sentence'] for item in batch])
            
            print(f"\n  å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(unique_items) + batch_size - 1)//batch_size}...")
            parsed = parse_text_with_ai(batch_text)
            all_parsed_sentences.extend(parsed)
        
        print(f"\nâœ“ å…±è§£æ {len(all_parsed_sentences)} ä¸ªå¥å­")
        print()
        
        # Step 4: ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        print("ğŸ”Š Step 4: ä½¿ç”¨ Edge-TTS ç”Ÿæˆè¯­éŸ³æ–‡ä»¶...")
        audio_files = await generate_all_audio(all_parsed_sentences)
        
        # Step 5: åˆ›å»º Anki å¡ç‰‡åŒ…
        print("ğŸ“¦ Step 5: ç”Ÿæˆ Anki å¡ç‰‡åŒ…...")
        apkg_file = create_anki_deck(all_parsed_sentences, audio_files)
        
        print()
        print("=" * 60)
        print("âœ… å…¨éƒ¨å®Œæˆï¼".center(60))
        print(f"è¾“å‡ºæ–‡ä»¶: {apkg_file}".center(60))
        print("=" * 60)
        
    except Exception as e:
        print()
        print("=" * 60)
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("=" * 60)
        raise
    
    finally:
        # Step 6: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        print()
        print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        cleanup_temp_files()


# ============= ç¨‹åºå…¥å£ =============
if __name__ == "__main__":
    asyncio.run(main())